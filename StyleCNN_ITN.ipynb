{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Artistic Style Transfer - Image Transformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gram matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.size()  # a=batch size(=1)\n",
    "        features = input.view(N, C, H * W)\n",
    "        G = torch.bmm(features, features.permute(0, 2, 1))\n",
    "        return G.div(C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Transformer Net (ITN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        # Upsampling Layers\n",
    "        self.deconv1 = UpsampleConvLayer(\n",
    "            128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(\n",
    "            64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "        # Non-linearities\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.deconv1(y)))\n",
    "        y = self.relu(self.in5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \"\"\"ResidualBlock\n",
    "    introduced in: https://arxiv.org/abs/1512.03385\n",
    "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \"\"\"UpsampleConvLayer\n",
    "    Upsamples the input and then does a convolution. This method gives better results\n",
    "    compared to ConvTranspose2d.\n",
    "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.Upsample(\n",
    "                mode='nearest', scale_factor=upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = self.upsample_layer(x_in)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Style CNN network with ITN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self):\n",
    "        super(StyleCNN, self).__init__()\n",
    "\n",
    "        # Initial configurations\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 5\n",
    "        self.style_weight = 1000\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "        # Image Transformer Net\n",
    "        self.itn = TransformerNet()\n",
    "        self.itn.to(device)\n",
    "        \n",
    "        # Loss network\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.itn.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss.cuda()\n",
    "            self.gram.cuda()\n",
    "\n",
    "    def train(self, content, style):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pastiche = self.itn(content) \n",
    "        pastiche.data.clamp_(0, 255)\n",
    "        pastiche_saved = pastiche.clone()\n",
    "        \n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "\n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "\n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_g = style_g.expand_as(pastiche_g)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return content_loss, style_loss, pastiche_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize((imsize, imsize)),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def save_images(input, paths):\n",
    "    N = input.size()[0]\n",
    "    images = input.data.clone().cpu()\n",
    "    for n in range(N):\n",
    "        image = images[n]\n",
    "        image = image.view(3, imsize, imsize)\n",
    "        image = unloader(image)\n",
    "        imageio.imwrite(paths[n], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch and Iter: 0, 84\n",
      "Content loss: 122.517135\n",
      "Style loss: 281.907546\n",
      "Epoch and Iter: 1, 84\n",
      "Content loss: 103.274354\n",
      "Style loss: 87.909144\n",
      "Epoch and Iter: 2, 84\n",
      "Content loss: 97.030778\n",
      "Style loss: 63.381110\n",
      "Epoch and Iter: 3, 84\n",
      "Content loss: 92.693492\n",
      "Style loss: 49.649481\n",
      "Epoch and Iter: 4, 84\n",
      "Content loss: 89.359270\n",
      "Style loss: 41.967311\n",
      "Epoch and Iter: 5, 84\n",
      "Content loss: 86.607182\n",
      "Style loss: 36.540488\n",
      "Epoch and Iter: 6, 84\n",
      "Content loss: 84.176983\n",
      "Style loss: 32.160616\n",
      "Epoch and Iter: 7, 84\n",
      "Content loss: 81.729950\n",
      "Style loss: 30.049597\n",
      "Epoch and Iter: 8, 84\n",
      "Content loss: 79.787665\n",
      "Style loss: 27.948957\n",
      "Epoch and Iter: 9, 84\n",
      "Content loss: 77.734398\n",
      "Style loss: 26.588030\n",
      "Epoch and Iter: 10, 84\n",
      "Content loss: 76.185359\n",
      "Style loss: 25.075062\n",
      "Epoch and Iter: 11, 84\n",
      "Content loss: 74.452669\n",
      "Style loss: 24.084234\n",
      "Epoch and Iter: 12, 84\n",
      "Content loss: 73.065024\n",
      "Style loss: 22.849250\n",
      "Epoch and Iter: 13, 84\n",
      "Content loss: 71.639664\n",
      "Style loss: 22.003104\n",
      "Epoch and Iter: 14, 84\n",
      "Content loss: 70.325697\n",
      "Style loss: 21.379698\n",
      "Epoch and Iter: 15, 84\n",
      "Content loss: 69.007365\n",
      "Style loss: 20.807455\n",
      "Epoch and Iter: 16, 84\n",
      "Content loss: 67.931021\n",
      "Style loss: 20.219905\n",
      "Epoch and Iter: 17, 84\n",
      "Content loss: 66.699381\n",
      "Style loss: 20.014566\n",
      "Epoch and Iter: 18, 84\n",
      "Content loss: 65.784947\n",
      "Style loss: 19.474410\n",
      "Epoch and Iter: 19, 84\n",
      "Content loss: 64.773673\n",
      "Style loss: 19.272395\n",
      "Epoch and Iter: 20, 84\n",
      "Content loss: 63.788712\n",
      "Style loss: 19.081683\n",
      "Epoch and Iter: 21, 84\n",
      "Content loss: 62.921609\n",
      "Style loss: 18.631686\n",
      "Epoch and Iter: 22, 84\n",
      "Content loss: 62.062297\n",
      "Style loss: 18.474362\n",
      "Epoch and Iter: 23, 84\n",
      "Content loss: 61.403241\n",
      "Style loss: 17.989093\n",
      "Epoch and Iter: 24, 84\n",
      "Content loss: 60.620491\n",
      "Style loss: 18.008055\n",
      "Epoch and Iter: 25, 84\n",
      "Content loss: 59.981570\n",
      "Style loss: 17.622124\n",
      "Epoch and Iter: 26, 84\n",
      "Content loss: 59.234888\n",
      "Style loss: 17.393262\n",
      "Epoch and Iter: 27, 84\n",
      "Content loss: 58.524709\n",
      "Style loss: 17.422242\n",
      "Epoch and Iter: 28, 84\n",
      "Content loss: 57.968552\n",
      "Style loss: 17.138675\n",
      "Epoch and Iter: 29, 84\n",
      "Content loss: 57.355810\n",
      "Style loss: 17.094417\n",
      "Epoch and Iter: 30, 84\n",
      "Content loss: 56.869742\n",
      "Style loss: 16.854824\n",
      "Epoch and Iter: 31, 84\n",
      "Content loss: 56.276856\n",
      "Style loss: 16.693309\n",
      "Epoch and Iter: 32, 84\n",
      "Content loss: 55.798347\n",
      "Style loss: 16.627250\n",
      "Epoch and Iter: 33, 84\n",
      "Content loss: 55.345155\n",
      "Style loss: 16.535942\n",
      "Epoch and Iter: 34, 84\n",
      "Content loss: 54.903176\n",
      "Style loss: 16.314580\n",
      "Epoch and Iter: 35, 84\n",
      "Content loss: 54.287997\n",
      "Style loss: 16.446956\n",
      "Epoch and Iter: 36, 84\n",
      "Content loss: 54.028979\n",
      "Style loss: 16.339686\n",
      "Epoch and Iter: 37, 84\n",
      "Content loss: 53.527663\n",
      "Style loss: 16.033731\n",
      "Epoch and Iter: 38, 84\n",
      "Content loss: 53.065014\n",
      "Style loss: 15.819125\n",
      "Epoch and Iter: 39, 84\n",
      "Content loss: 52.765169\n",
      "Style loss: 15.574702\n",
      "Epoch and Iter: 40, 84\n",
      "Content loss: 52.156054\n",
      "Style loss: 15.549416\n",
      "Epoch and Iter: 41, 84\n",
      "Content loss: 51.996119\n",
      "Style loss: 15.344289\n",
      "Epoch and Iter: 42, 84\n",
      "Content loss: 51.590665\n",
      "Style loss: 15.261762\n",
      "Epoch and Iter: 43, 84\n",
      "Content loss: 51.138680\n",
      "Style loss: 15.188357\n",
      "Epoch and Iter: 44, 84\n",
      "Content loss: 50.797999\n",
      "Style loss: 15.326517\n",
      "Epoch and Iter: 45, 84\n",
      "Content loss: 50.581279\n",
      "Style loss: 15.084776\n",
      "Epoch and Iter: 46, 84\n",
      "Content loss: 50.192703\n",
      "Style loss: 15.107159\n",
      "Epoch and Iter: 47, 84\n",
      "Content loss: 49.800826\n",
      "Style loss: 14.852698\n",
      "Epoch and Iter: 48, 84\n",
      "Content loss: 49.534053\n",
      "Style loss: 14.726070\n",
      "Epoch and Iter: 49, 84\n",
      "Content loss: 49.169307\n",
      "Style loss: 14.738186\n",
      "Epoch and Iter: 50, 84\n",
      "Content loss: 48.896153\n",
      "Style loss: 14.796332\n",
      "Epoch and Iter: 51, 84\n",
      "Content loss: 48.660072\n",
      "Style loss: 14.669099\n",
      "Epoch and Iter: 52, 84\n",
      "Content loss: 48.318713\n",
      "Style loss: 14.542646\n",
      "Epoch and Iter: 53, 84\n",
      "Content loss: 48.008116\n",
      "Style loss: 14.162245\n",
      "Epoch and Iter: 54, 84\n",
      "Content loss: 47.693517\n",
      "Style loss: 14.262003\n",
      "Epoch and Iter: 55, 84\n",
      "Content loss: 47.572735\n",
      "Style loss: 14.010437\n",
      "Epoch and Iter: 56, 84\n",
      "Content loss: 47.275965\n",
      "Style loss: 14.103090\n",
      "Epoch and Iter: 57, 84\n",
      "Content loss: 47.100668\n",
      "Style loss: 14.317766\n",
      "Epoch and Iter: 58, 84\n",
      "Content loss: 46.943156\n",
      "Style loss: 14.136116\n",
      "Epoch and Iter: 59, 84\n",
      "Content loss: 46.545040\n",
      "Style loss: 13.795678\n",
      "Epoch and Iter: 60, 84\n",
      "Content loss: 46.226007\n",
      "Style loss: 13.960093\n",
      "Epoch and Iter: 61, 84\n",
      "Content loss: 46.094651\n",
      "Style loss: 14.004195\n",
      "Epoch and Iter: 62, 84\n",
      "Content loss: 45.797249\n",
      "Style loss: 14.217830\n",
      "Epoch and Iter: 63, 84\n",
      "Content loss: 45.566495\n",
      "Style loss: 13.957920\n",
      "Epoch and Iter: 64, 84\n",
      "Content loss: 45.247712\n",
      "Style loss: 13.862065\n",
      "Epoch and Iter: 65, 84\n",
      "Content loss: 45.105235\n",
      "Style loss: 13.912434\n",
      "Epoch and Iter: 66, 84\n",
      "Content loss: 44.756036\n",
      "Style loss: 13.801705\n",
      "Epoch and Iter: 67, 84\n",
      "Content loss: 44.719735\n",
      "Style loss: 13.647643\n",
      "Epoch and Iter: 68, 84\n",
      "Content loss: 44.211164\n",
      "Style loss: 13.807585\n",
      "Epoch and Iter: 69, 84\n",
      "Content loss: 44.280382\n",
      "Style loss: 13.561831\n",
      "Epoch and Iter: 70, 84\n",
      "Content loss: 43.902687\n",
      "Style loss: 13.546859\n",
      "Epoch and Iter: 71, 84\n",
      "Content loss: 43.612333\n",
      "Style loss: 13.603130\n",
      "Epoch and Iter: 72, 84\n",
      "Content loss: 43.423679\n",
      "Style loss: 13.904704\n",
      "Epoch and Iter: 73, 84\n",
      "Content loss: 43.425816\n",
      "Style loss: 13.639562\n",
      "Epoch and Iter: 74, 84\n",
      "Content loss: 43.177376\n",
      "Style loss: 13.556610\n",
      "Epoch and Iter: 75, 84\n",
      "Content loss: 42.791516\n",
      "Style loss: 13.518462\n",
      "Epoch and Iter: 76, 84\n",
      "Content loss: 42.722144\n",
      "Style loss: 13.449498\n",
      "Epoch and Iter: 77, 84\n",
      "Content loss: 42.611123\n",
      "Style loss: 13.404841\n",
      "Epoch and Iter: 78, 84\n",
      "Content loss: 42.793719\n",
      "Style loss: 13.926482\n",
      "Epoch and Iter: 79, 84\n",
      "Content loss: 41.285559\n",
      "Style loss: 13.064414\n",
      "Epoch and Iter: 80, 84\n",
      "Content loss: 41.348421\n",
      "Style loss: 12.995653\n",
      "Epoch and Iter: 81, 84\n",
      "Content loss: 41.543705\n",
      "Style loss: 13.192499\n",
      "Epoch and Iter: 82, 84\n",
      "Content loss: 41.527910\n",
      "Style loss: 13.669531\n",
      "Epoch and Iter: 83, 84\n",
      "Content loss: 40.721561\n",
      "Style loss: 12.775623\n",
      "Epoch and Iter: 84, 84\n",
      "Content loss: 40.874083\n",
      "Style loss: 12.986188\n",
      "Epoch and Iter: 85, 84\n",
      "Content loss: 41.081778\n",
      "Style loss: 13.716677\n",
      "Epoch and Iter: 86, 84\n",
      "Content loss: 39.980863\n",
      "Style loss: 12.673829\n",
      "Epoch and Iter: 87, 84\n",
      "Content loss: 40.444381\n",
      "Style loss: 12.889547\n",
      "Epoch and Iter: 88, 84\n",
      "Content loss: 40.548048\n",
      "Style loss: 13.591161\n",
      "Epoch and Iter: 89, 84\n",
      "Content loss: 39.483504\n",
      "Style loss: 12.542853\n",
      "Epoch and Iter: 90, 84\n",
      "Content loss: 39.681200\n",
      "Style loss: 12.380190\n",
      "Epoch and Iter: 91, 84\n",
      "Content loss: 39.686639\n",
      "Style loss: 12.861484\n",
      "Epoch and Iter: 92, 84\n",
      "Content loss: 39.599704\n",
      "Style loss: 12.859445\n",
      "Epoch and Iter: 93, 84\n",
      "Content loss: 39.332715\n",
      "Style loss: 12.678203\n",
      "Epoch and Iter: 94, 84\n",
      "Content loss: 39.648387\n",
      "Style loss: 12.982334\n",
      "Epoch and Iter: 95, 84\n",
      "Content loss: 39.211542\n",
      "Style loss: 12.840264\n",
      "Epoch and Iter: 96, 84\n",
      "Content loss: 39.694535\n",
      "Style loss: 13.203827\n",
      "Epoch and Iter: 97, 84\n",
      "Content loss: 38.730037\n",
      "Style loss: 12.612198\n",
      "Epoch and Iter: 98, 84\n",
      "Content loss: 38.635278\n",
      "Style loss: 12.323206\n",
      "Epoch and Iter: 99, 84\n",
      "Content loss: 39.486825\n",
      "Style loss: 13.279929\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# CUDA Configurations\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Batch Size\n",
    "N = 4\n",
    "\n",
    "# Contents\n",
    "coco = datasets.ImageFolder(root='contents/', transform=loader)\n",
    "content_loader = torch.utils.data.DataLoader(coco, batch_size=N, shuffle=True)\n",
    "\n",
    "# Style\n",
    "style = load_image(\"styles/starry_night.jpg\").type(dtype)\n",
    "\n",
    "# Declare the network\n",
    "style_cnn = StyleCNN()\n",
    "   \n",
    "num_epochs = 100\n",
    "agg_content_loss = 0\n",
    "agg_style_loss = 0\n",
    "style_cnn.itn.train()\n",
    "interval = 85\n",
    "for epoch in range(num_epochs):\n",
    "    for i, content_batch in enumerate(content_loader):\n",
    "        content_batch = content_batch[0].type(dtype)\n",
    "        content_loss, style_loss, pastiches = style_cnn.train(content_batch, style)\n",
    "        \n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if i == len(content_loader)-1:\n",
    "            print(\"Epoch and Iter: %d, %d\"% (epoch, i))\n",
    "            print(\"Content loss: %f\" % (agg_content_loss/interval))\n",
    "            print(\"Style loss: %f\" % (agg_style_loss/interval))\n",
    "\n",
    "            path = \"outputs/pastiche_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(pastiches, paths)\n",
    "\n",
    "            path = \"outputs/content_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(content_batch, paths)\n",
    "            \n",
    "            agg_content_loss = 0\n",
    "            agg_style_loss = 0\n",
    "            style_cnn.itn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(\"contents/building.jpg\").type(dtype)\n",
    "pastiche = style_cnn.itn(content)\n",
    "pastiche.data.clamp_(0, 255)\n",
    "image = pastiche.data.clone().cpu()\n",
    "image = image.view(3, imsize, imsize)\n",
    "image = unloader(image)\n",
    "imageio.imwrite(\"outputs/pastiche_building.png\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
