{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Artistic Style Transfer - Image Transformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gram matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.size()  # a=batch size(=1)\n",
    "        features = input.view(N, C, H * W)\n",
    "        G = torch.bmm(features, features.permute(0, 2, 1))\n",
    "        return G.div(C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Transformer Net (ITN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.image_transformer_net import TransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Style CNN network with ITN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self):\n",
    "        super(StyleCNN, self).__init__()\n",
    "\n",
    "        # Initial configurations\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 2\n",
    "        self.style_weight = 1000\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "        # Image Transformer Net\n",
    "        self.itn = TransformerNet()\n",
    "        self.itn.to(device)\n",
    "        \n",
    "        # Loss network\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.itn.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss.cuda()\n",
    "            self.gram.cuda()\n",
    "\n",
    "    def train(self, content, style):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pastiche = self.itn(content) \n",
    "        pastiche.data.clamp_(0, 255)\n",
    "        pastiche_saved = pastiche.clone()\n",
    "        \n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "\n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "\n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_g = style_g.expand_as(pastiche_g)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return content_loss, style_loss, pastiche_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize((imsize, imsize)),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def save_images(input, paths):\n",
    "    N = input.size()[0]\n",
    "    images = input.data.clone().cpu()\n",
    "    for n in range(N):\n",
    "        image = images[n]\n",
    "        image = image.view(3, imsize, imsize)\n",
    "        image = unloader(image)\n",
    "        imageio.imwrite(paths[n], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Content loss: 34.042993\n",
      "Style loss: 103.598893\n",
      "Epoch: 1\n",
      "Content loss: 31.872838\n",
      "Style loss: 20.241852\n",
      "Epoch: 2\n",
      "Content loss: 29.599655\n",
      "Style loss: 14.596667\n",
      "Epoch: 3\n",
      "Content loss: 27.613654\n",
      "Style loss: 12.772306\n",
      "Epoch: 4\n",
      "Content loss: 25.887567\n",
      "Style loss: 11.440208\n",
      "Epoch: 5\n",
      "Content loss: 24.499462\n",
      "Style loss: 10.549997\n",
      "Epoch: 6\n",
      "Content loss: 23.344501\n",
      "Style loss: 9.886719\n",
      "Epoch: 7\n",
      "Content loss: 22.394160\n",
      "Style loss: 9.358080\n",
      "Epoch: 8\n",
      "Content loss: 21.661714\n",
      "Style loss: 8.901360\n",
      "Epoch: 9\n",
      "Content loss: 21.188854\n",
      "Style loss: 8.751525\n",
      "Epoch: 10\n",
      "Content loss: 20.603273\n",
      "Style loss: 8.464369\n",
      "Epoch: 11\n",
      "Content loss: 20.681054\n",
      "Style loss: 9.442226\n",
      "Epoch: 12\n",
      "Content loss: 21.740107\n",
      "Style loss: 11.763464\n",
      "Epoch: 13\n",
      "Content loss: 20.673017\n",
      "Style loss: 9.515477\n",
      "Epoch: 14\n",
      "Content loss: 22.327216\n",
      "Style loss: 12.884334\n",
      "Epoch: 15\n",
      "Content loss: 22.270103\n",
      "Style loss: 12.194686\n",
      "Epoch: 16\n",
      "Content loss: 20.633679\n",
      "Style loss: 8.653313\n",
      "Epoch: 17\n",
      "Content loss: 22.035872\n",
      "Style loss: 11.901758\n",
      "Epoch: 18\n",
      "Content loss: 23.443487\n",
      "Style loss: 16.731838\n",
      "Epoch: 19\n",
      "Content loss: 21.526156\n",
      "Style loss: 9.903312\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# CUDA Configurations\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Batch Size\n",
    "N = 4\n",
    "\n",
    "# Contents\n",
    "coco = datasets.ImageFolder(root='contents/', transform=loader)\n",
    "content_loader = torch.utils.data.DataLoader(coco, batch_size=N, shuffle=True)\n",
    "\n",
    "# Style\n",
    "style = load_image(\"styles/mosaic.jpg\").type(dtype)\n",
    "\n",
    "# Declare the network\n",
    "style_cnn = StyleCNN()\n",
    "   \n",
    "num_epochs = 20\n",
    "agg_content_loss = 0\n",
    "agg_style_loss = 0\n",
    "style_cnn.itn.train()\n",
    "interval = len(content_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, content_batch in enumerate(content_loader):\n",
    "        content_batch = content_batch[0].type(dtype)\n",
    "        content_loss, style_loss, pastiches = style_cnn.train(content_batch, style)\n",
    "        pastiches.data.clamp_(0, 1)\n",
    "        \n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if i == len(content_loader)-1:\n",
    "            print(\"Epoch: %d\" % (epoch))\n",
    "            print(\"Content loss: %f\" % (agg_content_loss/interval))\n",
    "            print(\"Style loss: %f\" % (agg_style_loss/interval))\n",
    "\n",
    "            path = \"outputs/pastiche_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(pastiches, paths)\n",
    "\n",
    "            path = \"outputs/content_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(content_batch, paths)\n",
    "            \n",
    "            agg_content_loss = 0\n",
    "            agg_style_loss = 0\n",
    "            style_cnn.itn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(\"contents/building.jpg\").type(dtype)\n",
    "pastiche = style_cnn.itn(content)\n",
    "pastiche.data.clamp_(0, 1)\n",
    "image = pastiche.data.clone().cpu()\n",
    "image = image.view(3, imsize, imsize)\n",
    "image = unloader(image)\n",
    "imageio.imwrite(\"outputs/pastiche_building.png\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.image_transformer_net import TransformerNet\n",
    "\n",
    "model = TransformerNet().to(device)\n",
    "model.load_state_dict(torch.load(\"models/mosaic.ckpt\"))\n",
    "content = load_image(\"contents/dog.jpg\").type(dtype)\n",
    "pastiche = model(content)\n",
    "pastiche.data.clamp_(0, 1)\n",
    "image = pastiche.data.clone().cpu()\n",
    "image = image.view(3, imsize, imsize)\n",
    "image = unloader(image)\n",
    "imageio.imwrite(\"outputs/pastiche_dog.png\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(style_cnn.itn.state_dict(), \"models/mosaic.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
