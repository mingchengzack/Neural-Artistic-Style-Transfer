{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Artistic Style Transfer - Image Transformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gram matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.size()  # a=batch size(=1)\n",
    "        features = input.view(N, C, H * W)\n",
    "        G = torch.bmm(features, features.permute(0, 2, 1))\n",
    "        return G.div(C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Transformer Net (ITN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.image_transformer_net import TransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Style CNN network with ITN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self):\n",
    "        super(StyleCNN, self).__init__()\n",
    "\n",
    "        # Initial configurations\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 5\n",
    "        self.style_weight = 2000\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "        # Image Transformer Net\n",
    "        self.itn = TransformerNet()\n",
    "        self.itn.to(device)\n",
    "        \n",
    "        # Loss network\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.itn.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss.cuda()\n",
    "            self.gram.cuda()\n",
    "\n",
    "    def train(self, content, style):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pastiche = self.itn(content) \n",
    "        pastiche.data.clamp_(0, 255)\n",
    "        pastiche_saved = pastiche.clone()\n",
    "        \n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "\n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "\n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_g = style_g.expand_as(pastiche_g)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return content_loss, style_loss, pastiche_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize((imsize, imsize)),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def save_images(input, paths):\n",
    "    N = input.size()[0]\n",
    "    images = input.data.clone().cpu()\n",
    "    for n in range(N):\n",
    "        image = images[n]\n",
    "        image = image.view(3, imsize, imsize)\n",
    "        image = unloader(image)\n",
    "        imageio.imwrite(paths[n], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /tmp/xdg-cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:11<00:00, 50.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Content loss: 119.060495\n",
      "Style loss: 178.111163\n",
      "Epoch: 1\n",
      "Content loss: 99.535315\n",
      "Style loss: 39.903877\n",
      "Epoch: 2\n",
      "Content loss: 87.794246\n",
      "Style loss: 31.542174\n",
      "Epoch: 3\n",
      "Content loss: 79.560889\n",
      "Style loss: 27.653427\n",
      "Epoch: 4\n",
      "Content loss: 72.944315\n",
      "Style loss: 24.977127\n",
      "Epoch: 5\n",
      "Content loss: 67.680464\n",
      "Style loss: 23.354009\n",
      "Epoch: 6\n",
      "Content loss: 63.401695\n",
      "Style loss: 21.814648\n",
      "Epoch: 7\n",
      "Content loss: 61.791762\n",
      "Style loss: 22.422623\n",
      "Epoch: 8\n",
      "Content loss: 59.005068\n",
      "Style loss: 20.446124\n",
      "Epoch: 9\n",
      "Content loss: 56.704083\n",
      "Style loss: 19.748710\n",
      "Epoch: 10\n",
      "Content loss: 54.935675\n",
      "Style loss: 19.187141\n",
      "Epoch: 11\n",
      "Content loss: 53.630494\n",
      "Style loss: 19.137463\n",
      "Epoch: 12\n",
      "Content loss: 52.595712\n",
      "Style loss: 18.646203\n",
      "Epoch: 13\n",
      "Content loss: 52.116055\n",
      "Style loss: 18.865801\n",
      "Epoch: 14\n",
      "Content loss: 51.528850\n",
      "Style loss: 18.595173\n",
      "Epoch: 15\n",
      "Content loss: 50.952658\n",
      "Style loss: 18.565839\n",
      "Epoch: 16\n",
      "Content loss: 51.316952\n",
      "Style loss: 19.063968\n",
      "Epoch: 17\n",
      "Content loss: 50.851666\n",
      "Style loss: 18.767341\n",
      "Epoch: 18\n",
      "Content loss: 50.035385\n",
      "Style loss: 17.899005\n",
      "Epoch: 19\n",
      "Content loss: 49.395402\n",
      "Style loss: 17.803235\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# CUDA Configurations\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Batch Size\n",
    "N = 4\n",
    "\n",
    "# Contents\n",
    "coco = datasets.ImageFolder(root='contents/', transform=loader)\n",
    "content_loader = torch.utils.data.DataLoader(coco, batch_size=N, shuffle=True)\n",
    "\n",
    "# Style\n",
    "style = load_image(\"styles/starry_night.jpg\").type(dtype)\n",
    "\n",
    "# Declare the network\n",
    "style_cnn = StyleCNN()\n",
    "   \n",
    "num_epochs = 20\n",
    "agg_content_loss = 0\n",
    "agg_style_loss = 0\n",
    "style_cnn.itn.train()\n",
    "interval = len(content_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, content_batch in enumerate(content_loader):\n",
    "        content_batch = content_batch[0].type(dtype)\n",
    "        content_loss, style_loss, pastiches = style_cnn.train(content_batch, style)\n",
    "        \n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if i == len(content_loader)-1:\n",
    "            print(\"Epoch: %d\" % (epoch))\n",
    "            print(\"Content loss: %f\" % (agg_content_loss/interval))\n",
    "            print(\"Style loss: %f\" % (agg_style_loss/interval))\n",
    "\n",
    "            path = \"outputs/pastiche_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(pastiches, paths)\n",
    "\n",
    "            path = \"outputs/content_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(content_batch, paths)\n",
    "            \n",
    "            agg_content_loss = 0\n",
    "            agg_style_loss = 0\n",
    "            style_cnn.itn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(\"contents/building.jpg\").type(dtype)\n",
    "pastiche = style_cnn.itn(content)\n",
    "pastiche.data.clamp_(0, 255)\n",
    "image = pastiche.data.clone().cpu()\n",
    "image = image.view(3, imsize, imsize)\n",
    "image = unloader(image)\n",
    "imageio.imwrite(\"outputs/pastiche_building.png\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(style_cnn.itn.state_dict(), \"models/styleCNN_ITN_ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
