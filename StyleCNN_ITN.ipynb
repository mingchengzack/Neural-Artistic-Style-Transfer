{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Artistic Style Transfer - Image Transformation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gram matrix layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.size()  # a=batch size(=1)\n",
    "        features = input.view(N, C, H * W)\n",
    "        G = torch.bmm(features, features.permute(0, 2, 1))\n",
    "        return G.div(C * H * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Transformer Net (ITN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        # Upsampling Layers\n",
    "        self.deconv1 = UpsampleConvLayer(\n",
    "            128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(\n",
    "            64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "        # Non-linearities\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.deconv1(y)))\n",
    "        y = self.relu(self.in5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \"\"\"ResidualBlock\n",
    "    introduced in: https://arxiv.org/abs/1512.03385\n",
    "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \"\"\"UpsampleConvLayer\n",
    "    Upsamples the input and then does a convolution. This method gives better results\n",
    "    compared to ConvTranspose2d.\n",
    "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample_layer = torch.nn.Upsample(\n",
    "                mode='nearest', scale_factor=upsample)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = self.upsample_layer(x_in)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Style CNN network with ITN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self):\n",
    "        super(StyleCNN, self).__init__()\n",
    "\n",
    "        # Initial configurations\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 5\n",
    "        self.style_weight = 2000\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "        # Image Transformer Net\n",
    "        self.itn = TransformerNet()\n",
    "        self.itn.to(device)\n",
    "        \n",
    "        # Loss network\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.itn.parameters(), lr=1e-4)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss.cuda()\n",
    "            self.gram.cuda()\n",
    "\n",
    "    def train(self, content, style):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        pastiche = self.itn(content) \n",
    "        pastiche.data.clamp_(0, 255)\n",
    "        pastiche_saved = pastiche.clone()\n",
    "        \n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "\n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "\n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_g = style_g.expand_as(pastiche_g)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return content_loss, style_loss, pastiche_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Resize((imsize, imsize)),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def load_image(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def save_images(input, paths):\n",
    "    N = input.size()[0]\n",
    "    images = input.data.clone().cpu()\n",
    "    for n in range(N):\n",
    "        image = images[n]\n",
    "        image = image.view(3, imsize, imsize)\n",
    "        image = unloader(image)\n",
    "        imageio.imwrite(paths[n], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Content loss: 117.424165\n",
      "Style loss: 226.510260\n",
      "Epoch: 1\n",
      "Content loss: 101.659600\n",
      "Style loss: 42.834743\n",
      "Epoch: 2\n",
      "Content loss: 89.771959\n",
      "Style loss: 33.898719\n",
      "Epoch: 3\n",
      "Content loss: 81.693906\n",
      "Style loss: 29.494100\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# CUDA Configurations\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Batch Size\n",
    "N = 4\n",
    "\n",
    "# Contents\n",
    "coco = datasets.ImageFolder(root='contents/', transform=loader)\n",
    "content_loader = torch.utils.data.DataLoader(coco, batch_size=N, shuffle=True)\n",
    "\n",
    "# Style\n",
    "style = load_image(\"styles/starry_night.jpg\").type(dtype)\n",
    "\n",
    "# Declare the network\n",
    "style_cnn = StyleCNN()\n",
    "   \n",
    "num_epochs = 5\n",
    "agg_content_loss = 0\n",
    "agg_style_loss = 0\n",
    "style_cnn.itn.train()\n",
    "interval = len(content_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, content_batch in enumerate(content_loader):\n",
    "        content_batch = content_batch[0].type(dtype)\n",
    "        content_loss, style_loss, pastiches = style_cnn.train(content_batch, style)\n",
    "        \n",
    "        agg_content_loss += content_loss.item()\n",
    "        agg_style_loss += style_loss.item()\n",
    "\n",
    "        if i == len(content_loader)-1:\n",
    "            print(\"Epoch: %d\" % (epoch))\n",
    "            print(\"Content loss: %f\" % (agg_content_loss/interval))\n",
    "            print(\"Style loss: %f\" % (agg_style_loss/interval))\n",
    "\n",
    "            path = \"outputs/pastiche_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(pastiches, paths)\n",
    "\n",
    "            path = \"outputs/content_%d_\" % (epoch)\n",
    "            paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "            save_images(content_batch, paths)\n",
    "            \n",
    "            agg_content_loss = 0\n",
    "            agg_style_loss = 0\n",
    "            style_cnn.itn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(\"contents/building.jpg\").type(dtype)\n",
    "pastiche = style_cnn.itn(content)\n",
    "pastiche.data.clamp_(0, 255)\n",
    "image = pastiche.data.clone().cpu()\n",
    "image = image.view(3, imsize, imsize)\n",
    "image = unloader(image)\n",
    "imageio.imwrite(\"outputs/pastiche_building.png\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
